# ğŸ‘€VisionPartner

<div align="center">

![VisionPartner Logo](https://img.shields.io/badge/VisionPartner-AI%20Navigation%20Assistant-blue?style=for-the-badge)

**Real-Time AI-Powered Navigation Assistant for the Visually Impaired**

[![Flutter](https://img.shields.io/badge/Flutter-02569B?style=flat&logo=flutter&logoColor=white)](https://flutter.dev)
[![TensorFlow](https://img.shields.io/badge/TensorFlow_Lite-FF6F00?style=flat&logo=tensorflow&logoColor=white)](https://www.tensorflow.org/lite)
[![Google Gemini](https://img.shields.io/badge/Google_Gemini-4285F4?style=flat&logo=google&logoColor=white)](https://ai.google.dev)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[Features](#-features) â€¢ [Demo](#-demo) â€¢ [Tech Stack](#-tech-stack) â€¢ [Architecture](#-architecture) â€¢ [Getting Started](#-getting-started) â€¢ [Roadmap](#-roadmap)

</div>

---

## ğŸ“– Overview

VisionPartner transforms smartphones into intelligent navigation companions for visually impaired individuals. Unlike traditional object detection apps that simply list what's around you, VisionPartner provides **contextual navigation commands** that tell you exactly how to move safely.

### The Problem

Over 2.2 billion people worldwide with visual impairments face daily challenges navigating even familiar environments:
- White canes detect ground-level obstacles but miss overhead hazards and moving objects
- Guide dogs are expensive ($50,000+), require years of training, and are accessible to less than 2% of those who need them
- Existing apps provide object labels without actionable guidance
- GPS navigation fails indoors and can't detect real-time obstacles

### Our Solution

VisionPartner acts as a **real-time co-pilot**, combining advanced AI scene understanding with practical movement instructions delivered through both **audio and haptic feedback**.

**Instead of saying:** *"Chair detected"*  
**We say:** *"Obstacle on your right â€” shift left"*

---

## âœ¨ Features

### ğŸ¯ Core Capabilities

- **ğŸ¤– Intelligent Scene Understanding** - Powered by Google Gemini Vision API for contextual awareness
- **âš¡ Real-Time Obstacle Detection** - On-device TensorFlow Lite for instant hazard identification
- **ğŸ§­ Directional Navigation Commands** - Actionable instructions: "move left", "stop", "stairs ahead"
- **ğŸ”Š Multi-Modal Feedback** 
  - Audio guidance via text-to-speech
  - Haptic vibration patterns for silent navigation
- **ğŸ“¶ Offline Core Functionality** - Essential safety features work without internet
- **ğŸ¯ Smart Prioritization** - Filters noise, highlights urgent hazards
- **â™¿ Accessibility-First Design** - Fully operable without visual interface

### ğŸš¨ Hazard Detection

- Stairs and level changes
- Doors and openings
- Moving vehicles
- Pedestrian traffic
- Narrow passages
- Overhead obstacles
- Ground hazards

---

## ğŸ¥ Demo

> **Coming Soon**: Video demonstration of VisionPartner in action

<!--
### Screenshots
[Add screenshots of the app in use]
-->

---

## ğŸ› ï¸ Tech Stack

### Frontend
- **Flutter** - Cross-platform mobile framework
- **Dart** - Programming language

### AI & Computer Vision
- **Google Gemini Vision API** - Advanced scene understanding and context analysis
- **TensorFlow Lite** - On-device object detection
- **Pre-trained Models** - MobileNet/YOLO optimized for mobile inference

### Device Integration
- **Camera Package** - Real-time video stream capture
- **Flutter TTS** - Text-to-speech engine
- **Vibration Package** - Haptic feedback control

### Development Tools
- **Git & GitHub** - Version control
- **VS Code / Android Studio** - IDEs
- **Flutter DevTools** - Debugging and performance

---

## ğŸ—ï¸ Architecture

### System Flow

```
ğŸ“± Camera Capture â†’ ğŸ¤– Dual AI Processing â†’ ğŸ§  Navigation Engine â†’ ğŸ”Š Multi-Modal Feedback â†’ ğŸ‘¤ User
```

### Component Breakdown

#### 1ï¸âƒ£ **Input Layer**
- Continuous camera frame capture
- Real-time video stream processing

#### 2ï¸âƒ£ **AI Processing Layer**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Processing (Gemini Vision)   â”‚  â†’ Rich scene context
â”‚  â€¢ Scene understanding              â”‚  â†’ Spatial relationships
â”‚  â€¢ Movement patterns                â”‚  â†’ Risk assessment
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  On-Device (TensorFlow Lite)        â”‚  â†’ Instant detection
â”‚  â€¢ Fast obstacle detection          â”‚  â†’ Offline capability
â”‚  â€¢ Immediate hazard alerts          â”‚  â†’ Low latency
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3ï¸âƒ£ **Intelligence Layer**
- **Data Fusion** - Combines cloud + local results
- **Navigation Reasoning Engine**
  - Filters irrelevant objects
  - Prioritizes hazards by urgency
  - Calculates spatial positioning
  - Generates directional commands

#### 4ï¸âƒ£ **Output Layer**
- **Audio Feedback** - Clear spoken instructions
- **Haptic Feedback** - Directional vibration patterns
  - Left buzz â†’ Move left
  - Right buzz â†’ Move right
  - Long buzz â†’ Stop
  - Rapid pulses â†’ Immediate danger

---

## ğŸš€ Getting Started

### Prerequisites

- Flutter SDK (3.0 or higher)
- Android Studio / Xcode
- Google Cloud account (for Gemini API)
- Physical device for testing (camera required)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/visionpartner.git
cd visionpartner
```

2. **Install dependencies**
```bash
flutter pub get
```

3. **Configure API Keys**

Create a `.env` file in the root directory:
```env
GEMINI_API_KEY=your_gemini_api_key_here
```

4. **Run the app**
```bash
flutter run
```

### Building for Production

**Android:**
```bash
flutter build apk --release
```

**iOS:**
```bash
flutter build ios --release
```

---

## ğŸ“± Usage

1. **Launch the app** - VisionPartner starts automatically
2. **Hold phone forward** - Camera should face your walking direction
3. **Listen for guidance** - Audio and vibration will guide you
4. **Adjust settings** - Voice control for volume, speech rate, vibration intensity

### Vibration Patterns

| Pattern | Meaning |
|---------|---------|
| Single left buzz | Shift left |
| Single right buzz | Shift right |
| Long continuous | Stop immediately |
| Rapid pulses | Danger ahead |
| Double buzz | Path clear |

---

## ğŸ—ºï¸ Roadmap

### âœ… Phase 1 - Hackathon MVP (Current)
- [x] Real-time camera processing
- [x] Gemini Vision API integration
- [x] On-device object detection
- [x] Audio + haptic feedback
- [x] Core navigation commands

### ğŸ”„ Phase 2 - User Testing (1-3 months)
- [ ] Beta testing with visually impaired users
- [ ] Performance optimization
- [ ] Battery efficiency improvements
- [ ] Enhanced detection accuracy

### ğŸ¯ Phase 3 - Feature Expansion (3-6 months)
- [ ] Indoor mapping and pathfinding
- [ ] Landmark recognition (ATMs, bus stops, signs)
- [ ] Text and signboard reading (OCR)
- [ ] Crosswalk signal detection
- [ ] Multi-language support

### ğŸŒŸ Phase 4 - Advanced Features (6-12 months)
- [ ] Public transit integration
- [ ] Crowd-sourced obstacle database
- [ ] Machine learning personalization
- [ ] Spatial audio guidance (3D sound)

### ğŸš€ Phase 5 - Long-Term Vision (1-2 years)
- [ ] Smart glasses integration
- [ ] Smart city infrastructure connection
- [ ] Collaborative navigation network
- [ ] AR overlays for low-vision users

---

## ğŸ¤ Contributing

We welcome contributions from the community! Whether you're a developer, designer, or accessibility advocate, there's a place for you.

### How to Contribute

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### Areas We Need Help

- ğŸ§ª Testing with visually impaired users
- ğŸ¨ UI/UX improvements for accessibility
- ğŸ¤– ML model optimization
- ğŸŒ Internationalization and localization
- ğŸ“š Documentation and tutorials
- â™¿ Accessibility consulting

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.



---

## ğŸ™ Acknowledgments

- Google Gemini Team for providing advanced vision AI capabilities
- TensorFlow team for mobile ML frameworks
- Flutter community for excellent cross-platform tools
- Accessibility advocates who provided valuable insights
- All beta testers and contributors

---

## ğŸ“ Contact

**Project Link:** [https://github.com/yourusername/visionpartner](https://github.com/yourusername/visionpartner)

**Email:** your.email@example.com

**Demo Video:** [Coming Soon]

---

## ğŸŒŸ Support the Project

If VisionPartner helps you or someone you know, please â­ star this repository to help others discover it!

---

<div align="center">

**Making the world navigable for everyone, one step at a time.**

![Built with Flutter](https://img.shields.io/badge/Built_with-Flutter-02569B?style=flat&logo=flutter)
![Powered by AI](https://img.shields.io/badge/Powered_by-AI-FF6F00?style=flat&logo=tensorflow)
![Accessibility First](https://img.shields.io/badge/Accessibility-First-4CAF50?style=flat&logo=accessibility)

</div>
